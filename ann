ANN

from tensorflow import keras
from keras.datasets import mnist     # MNIST dataset is included in Keras
(X_train, y_train), (X_test, y_test) = mnist.load_data()

print("X_train shape", X_train.shape)
print("y_train shape", y_train.shape)
print("X_test shape", X_test.shape)
print("y_test shape", y_test.shape)

# Plot first few images
import matplotlib.pyplot as plt
for i in range(9):
	# define subplot
	plt.subplot(3,3,i+1) # 3 rows, 3 col, pos
	# plot raw pixel data
	plt.imshow(X_train[i], cmap='gray')
# show the figure
plt.show()

# Each pixel is an 8-bit integer from 0-255 (0 is full black, 255 is full white)
# single-channel pixel or monochrome image
X_train[i][10:20,10:20]

# reshape 28 x 28 matrices into 784-length vectors
X_train = X_train.reshape(60000, 784)
X_test = X_test.reshape(10000, 784)

# normalize each value for each pixel for the entire vector for each input
# change integers to 32-bit floating point numbers
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
# normalize by dividing by largest pixel value
X_train /= 255
X_test /= 255

print("Training matrix shape", X_train.shape)
print("Testing matrix shape", X_test.shape)

from keras.models import Sequential
from keras.layers import Dense

mdl = Sequential()

# Assuming input shape is (784,)
input_shape = (784,)

# Input layer with 64 units and relu activation
mdl.add(Dense(64, input_shape=input_shape, activation='relu'))

# Hidden layer with 32 units and relu activation
mdl.add(Dense(32, activation='relu'))

# Output layer with 10 units and softmax activation
mdl.add(Dense(10, activation='softmax'))

# Compile model
mdl.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Visualize the model
from keras.utils import plot_model
plot_model(mdl, show_shapes=True, show_layer_names=False)

# Display model summary
mdl.summary()

#one hot encoding
from keras.utils import to_categorical
y_train1 = to_categorical(y_train)
y_test1 = to_categorical(y_test)
print(y_test[6])
print(y_test1[6,:])

# Train the model
epochs=10
batch = 64
mdl.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
history = mdl.fit(X_train, y_train1,epochs=epochs, batch_size=batch,verbose=1, validation_data=(X_test, y_test1))

#plot
epochRange = range(1,epochs+1)
plt.plot(epochRange,history.history['loss'])
plt.plot(epochRange,history.history['val_loss'])
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid()
plt.xlim((1,epochs))
plt.legend(['Train','Test'])
plt.show()

plt.plot(epochRange,history.history['accuracy'])
plt.plot(epochRange,history.history['val_accuracy'])
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.grid()
plt.xlim((1,epochs))
plt.legend(['Train','Test'])
plt.show()

#Performance metrics
import numpy as np
yhat_test_mdl_prob = mdl.predict(X_test);
yhat_test_mdl = np.argmax(yhat_test_mdl_prob,axis=-1)
print(yhat_test_mdl_prob[0])
print(yhat_test_mdl[0:10])
print(y_test[0:10])

from sklearn.metrics import accuracy_score
print('Accuracy:')
print(float(accuracy_score(y_test, yhat_test_mdl))*100,'%')

from sklearn.metrics import confusion_matrix
print('Confusion Matrix:')
print(confusion_matrix(y_test, yhat_test_mdl))



