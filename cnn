from tensorflow import keras
from keras.datasets import mnist     # MNIST dataset is included in Keras
(X_train, y_train), (X_test, y_test) = mnist.load_data()

print("X_train shape", X_train.shape)
print("y_train shape", y_train.shape)
print("X_test shape", X_test.shape)
print("y_test shape", y_test.shape)

# Visualize any random image
import matplotlib.pyplot as plt
i=50
plt.imshow(X_train[i], cmap='gray');

# Single-channel input data (grey-scale)
# First apply convolutions then flatten

X_train = X_train.reshape(60000, 28, 28, 1) # single-channel input
X_test = X_test.reshape(10000, 28, 28, 1)

X_train = X_train.astype('float32')         # change integers to 32-bit floating point numbers
X_test = X_test.astype('float32')

X_train /= 255                              # min-max normalization
X_test /= 255

print("Training matrix shape", X_train.shape)
print("Testing matrix shape", X_test.shape)

#CNN
from keras import backend as K
from keras import __version__

print('Using Keras version:', __version__, 'backend:', K.backend())
# import cnn layers
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense
import tensorflow as tf
model=Sequential()
input_shape = (28, 28, 1)  # Assuming input images are grayscale with dimensions 28x28
num_classes = 10  # Assuming a classification task with 10 classes
# Convolution Layer 1: 8 filters, kernel size 3x3, relu activation, valid padding, stride 1
model.add(Conv2D(8, kernel_size=(3, 3), activation='relu', padding='valid', strides=(1, 1), input_shape=input_shape))

# MaxPooling: pool size 2, stride 2
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

# Convolution Layer 2: 16 filters, kernel size 3x3, relu activation, valid padding, stride 1
model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', padding='valid', strides=(1, 1)))

# MaxPooling: pool size 2, stride 2
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

# Flatten final feature matrix into a 1d array
model.add(Flatten())

# Fully Connected Layer: 64 units and relu activation
model.add(Dense(64, activation='relu'))

# Dropout layer, 0.2 rate
model.add(Dropout(0.2))

# Final output dense Layer
model.add(Dense(num_classes, activation='softmax'))

# Compile the model with sparse_categorical_crossentropy loss
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model.summary()

# Conv1: 3x3 kernels, one for each the single channel, 8 such filters and 8 biases
print('Conv1: ',3*3*1*8 + 8)
# Conv2: 3x3 kernels, one for each of the 8 channels, 16 such filters and 16 biases
print('Conv2: ',3*3*8*16 + 16)
# input to dense layer
print('Flatten:', 5*5*16)
# 400 inputs, 1 bias connected to each of 64 units in dense layer
print('Dense1: ',400*64+64)
# 64 inputs, 1 bias connected to each of 10 units in output layer
print('Dense2: ',64*10+10)

# Visualize the model
from keras.utils import plot_model
plot_model(model, show_shapes=True, show_layer_names=False)

# Train the model
batch_size=128
epochs=10
hist = model.fit(X_train, y_train,epochs=epochs,batch_size=batch_size,verbose=1,validation_split=0.2)

score = model.evaluate(X_test, y_test, verbose = 0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

# make one prediction
print('Actual class:',y_test[0])
print('Class Probabilities:')
model.predict(X_test[0].reshape(1,28,28,1))

import numpy as np
yhat_test = np.argmax(model.predict(X_test),axis=-1)
print(yhat_test[0:10])
print(y_test[0:10])



from sklearn.metrics import accuracy_score
print('Accuracy:')
print(float(accuracy_score(y_test, yhat_test))*100,'%')
     


from sklearn.metrics import confusion_matrix
print('Confusion Matrix:')
print(confusion_matrix(y_test, yhat_test))
     
